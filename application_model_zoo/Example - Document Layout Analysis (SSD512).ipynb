{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Tessellate-Imaging/Monk_Object_Detection/blob/master/application_model_zoo/Example%20-%20Document%20Layout%20Analysis%20(SSD512).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Layout Analysis Using SSD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the network:\n",
    "1. Paper on SSD: https://arxiv.org/abs/1512.02325\n",
    "\n",
    "2. Blog-1 on SSD: https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11\n",
    "\n",
    "3. Blog-2 on SSD: https://medium.com/@jonathan_hui/ssd-object-detection-single-shot-multibox-detector-for-real-time-processing-9bd8deac0e06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "### 1. Installation Instructions\n",
    "### 2. Use trained Model for Document Layout Analysis\n",
    "### 3. How to train using PRImA Layout Analysis Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MXRCNN pipeline of Monk Object Detection Library has been used for implementing this model. \n",
    "\n",
    " - After some comparisons, it was found out that VGG16 performs better than ResNet101 for object detection task using FasterRCNN, so VGG16 is chosen for the backend. Firstly, the dataset was converted from Monk Format to COCO Format. \n",
    " - The model was trained for 3 epochs with learning rate of 0.005 and then for 3 more epochs with learning rate of 0.001. The image was preprocessed to smaller size (min 300px, max 500px) and normalsied using mean and standard deviation calculated in preprocessing notebook. \n",
    " - Batch size has been kept at 2 because more than that was causing CUDAOutOfMemory error. It achieved RPN Accuracy=0.807714 and RCNN Accuracy= 0.750662."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "\n",
    "- Run these commands\n",
    "\n",
    "    - git clone https://github.com/Tessellate-Imaging/Monk_Object_Detection.git\n",
    "\n",
    "    - cd Monk_Object_Detection/1_gluoncv_finetune/installation\n",
    "\n",
    "- Select the right requirements file and run\n",
    "\n",
    "    - cat requirements_cuda10.1.txt | xargs -n 1 -L 1 pip install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/Tessellate-Imaging/Monk_Object_Detection.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For colab use the command below\n",
    "#! cd Monk_Object_Detection/1_gluoncv_finetune/installation && cat requirements_colab.txt | xargs -n 1 -L 1 pip install\n",
    "\n",
    "\n",
    "# For Local systems and cloud select the right CUDA version\n",
    "!cd Monk_Object_Detection/1_gluoncv_finetune/installation && cat requirements_cuda10.1.txt | xargs -n 1 -L 1 pip install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Already Trained Model for Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"Monk_Object_Detection/1_gluoncv_finetune/lib/\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference_prototype import Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1E6T7RKGwy-v1MUxVJm-rxt5XcRyr2SQ7' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1E6T7RKGwy-v1MUxVJm-rxt5XcRyr2SQ7\" -O obj_dla_ssd512_trained.zip && rm -rf /tmp/cookies.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! unzip -qq obj_dla_ssd512_trained.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ssd_512_vgg16_atrous_coco\";\n",
    "params_file = \"dla_ssd512/dla_ssd512-vgg16.params\";\n",
    "class_list = [\"paragraph\", \"heading\", \"credit\", \"footer\", \"drop-capital\", \"floating\", \"noise\", \"maths\", \"header\", \"caption\", \"image\", \"linedrawing\", \"graphics\", \"fname\", \"page-number\", \"chart\", \"separator\", \"table\"];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtf = Infer(model_name, params_file, class_list, use_gpu=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1VEkfJuicIr-STIqYOI-UruDDttWGXNxw' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1VEkfJuicIr-STIqYOI-UruDDttWGXNxw\" -O Test_Images.zip && rm -rf /tmp/cookies.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! unzip -qq Test_Images.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name = \"Test_Images/test1.jpg\"; \n",
    "visualize = True;\n",
    "thresh = 0.3;\n",
    "output = gtf.run(img_name, visualize=visualize, thresh=thresh);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name = \"Test_Images/test2.jpg\"; \n",
    "visualize = True;\n",
    "thresh = 0.3;\n",
    "output = gtf.run(img_name, visualize=visualize, thresh=thresh);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name = \"Test_Images/test3.jpg\"; \n",
    "visualize = True;\n",
    "thresh = 0.4;\n",
    "output = gtf.run(img_name, visualize=visualize, thresh=thresh);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Your Own Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Credits\n",
    "- https://www.primaresearch.org/datasets/Layout_Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1iBfafT1WHAtKAW0a1ifLzvW5f0ytm2i_' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1iBfafT1WHAtKAW0a1ifLzvW5f0ytm2i_\" -O PRImA_Layout_Analysis_Dataset.zip && rm -rf /tmp/cookies.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! unzip -qq PRImA_Layout_Analysis_Dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library for Data Augmentation\n",
    "Refer to https://github.com/albumentations-team/albumentations for more details\n",
    "\n",
    "### Data Preprocessing\n",
    "\n",
    "  - Normalisation: Calculated Mean & Standard deviation of training images (3 images taken out of dataset for inference) to feed into model for normalisation (used in FasterRCNN).\n",
    "\n",
    "  - Format Conversion: TIFF format was causing problems in data augmentation and training on TIFF images was more than 5x slower than JPEG format images because of their huge size. Therefore, TIFF images were converted to JPEG format images.\n",
    "\n",
    "  - Selective Data Augmentation: In the raw dataset 4750+ paragraph type objects and only 10-30 frames, graphics, etc. type objects which led to huge bias in dataset. To generate more data and to decrease bias, a customised function has been implemented from the scratch. This function produces random translational augmentated images of only those images which have minority classes in them. Using this function, the dataset size increased from 475 images to 1783 images. If data augmentation has been done on every image, there would have been 24000+ paragraphs in the dataset whereas there are 19568 now, which slightly improves the bias. (Exact numbers are in the data preprocessing notebook). The augmented images had to be saved in the dataset because augmentation function couldn't be called on the go.\n",
    "\n",
    "  - Conversion to VOC to Monk type- Coversion so that Monk format can be later used in SSD Model, converted to yolo type for Yolo model, and COCO format for FasterRCNN Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import xmltodict\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from pycocotools.coco import COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"PRImA Layout Analysis Dataset/\";\n",
    "img_dir = \"Images/\";\n",
    "anno_dir = \"XML/\";\n",
    "final_root_dir=\"Document_Layout_Analysis/\" #Directory for jpeg and augmented images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(final_root_dir):\n",
    "    os.makedirs(final_root_dir)\n",
    "\n",
    "if not os.path.exists(final_root_dir+img_dir):\n",
    "    os.makedirs(final_root_dir+img_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TIFF Image Format to JPEG Image Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in glob.glob(root_dir+img_dir+'*.tif'):\n",
    "    im = Image.open(name)\n",
    "    name = str(name).rstrip(\".tif\")\n",
    "    name = str(name).lstrip(root_dir)\n",
    "    name = str(name).lstrip(img_dir)\n",
    "    im.save(final_root_dir+ img_dir+ name + '.jpg', 'JPEG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format Conversion and Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As most part of a document is text, there were far more paragraphs in the dataset than there were other labels such as tables or graphs. To handle this huge bias in the dataset, we augmented only those document images which had one of these minority labels in them. For example, if the document only had paragraphs and images, then we didn’t augment it. But if it had tables, charts, graphs or any other minority label, we augmented that image by many folds. This process helped in reducing the bias in the dataset by around 25%. This selection and augmentation has been done during the format conversion from VOC to Monk Format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given format- VOC Format\n",
    "\n",
    "### Dataset Directory Structure\n",
    "\n",
    "    ./PRImA Layout Analysis Dataset/ (root_dir)\n",
    "          |\n",
    "          |-----------Images (img_dir)\n",
    "          |              |\n",
    "          |              |------------------img1.jpg\n",
    "          |              |------------------img2.jpg\n",
    "          |              |------------------.........(and so on)\n",
    "          |\n",
    "          |\n",
    "          |-----------Annotations (anno_dir)\n",
    "          |              |\n",
    "          |              |------------------img1.xml\n",
    "          |              |------------------img2.xml\n",
    "          |              |------------------.........(and so on)\n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Format- Monk Format\n",
    "\n",
    "### Dataset Directory Structure\n",
    "\n",
    "    ./Document_Layout_Analysis/ (final_root_dir)\n",
    "          |\n",
    "          |-----------Images (img_dir)\n",
    "          |              |\n",
    "          |              |------------------img1.jpg\n",
    "          |              |------------------img2.jpg\n",
    "          |              |------------------.........(and so on)\n",
    "          |\n",
    "          |\n",
    "          |-----------train_labels.csv (anno_file)\n",
    "          \n",
    "          \n",
    "### Annotation file format\n",
    "\n",
    "           | Id         | Labels                                 |\n",
    "           | img1.jpg   | x1 y1 x2 y2 label1 x1 y1 x2 y2 label2  |\n",
    "           \n",
    "- Labels:  xmin ymin xmax ymax label\n",
    "- xmin, ymin - top left corner of bounding box\n",
    "- xmax, ymax - bottom right corner of bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(root_dir + anno_dir);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = [];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentData(fname, boxes):\n",
    "    image = cv2.imread(final_root_dir+img_dir+fname)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    " \n",
    "    transform = A.Compose([\n",
    "        A.IAAPerspective(p=0.7),   \n",
    "        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=5, p=0.5),\n",
    "        A.IAAAdditiveGaussianNoise(),\n",
    "        A.ChannelShuffle(),\n",
    "        A.RandomBrightnessContrast(),\n",
    "        A.RGBShift(p=0.8),\n",
    "        A.HueSaturationValue(p=0.8)\n",
    "        ], bbox_params=A.BboxParams(format='pascal_voc', min_visibility=0.2))\n",
    "    \n",
    "    for i in range(1, 9):\n",
    "        label=\"\"\n",
    "        transformed = transform(image=image, bboxes=boxes)\n",
    "        transformed_image = transformed['image']\n",
    "        transformed_bboxes = transformed['bboxes']\n",
    "        #print(transformed_bboxes)\n",
    "        flag=False\n",
    "        for box in transformed_bboxes:\n",
    "            x_min, y_min, x_max, y_max, class_name = box\n",
    "            if(xmax<=xmin or ymax<=ymin):\n",
    "                flag=True\n",
    "                break\n",
    "            label+= str(int(x_min))+' '+str(int(y_min))+' '+str(int(x_max))+' '+str(int(y_max))+' '+class_name+' '\n",
    "                        \n",
    "        if(flag):\n",
    "            continue\n",
    "        cv2.imwrite(final_root_dir+img_dir+str(i)+fname, transformed_image)\n",
    "        label=label[:-1]\n",
    "        combined.append([str(i) + fname, label])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VOC to Monk Format Conversion\n",
    "Applying Data Augmentation only on those images which contain atleast 1 minority class so as to reduce bias in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#label generation for csv\n",
    "for i in tqdm(range(len(files))):\n",
    "    box=[];\n",
    "    augment=False;\n",
    "    annoFile = root_dir + anno_dir + files[i];\n",
    "    f = open(annoFile, 'r');\n",
    "    my_xml = f.read();\n",
    "    anno= dict(dict(dict(xmltodict.parse(my_xml))['PcGts'])['Page'])\n",
    "    fname=\"\"\n",
    "    for j in range(len(files[i])):\n",
    "        if((files[i][j])>='0' and files[i][j]<='9'):\n",
    "            fname+=files[i][j];\n",
    "    fname+=\".jpg\"\n",
    "    image = cv2.imread(final_root_dir+img_dir+fname)\n",
    "    height, width = image.shape[:2]    \n",
    "    label_str = \"\"\n",
    "    for key in anno.keys():\n",
    "        if(key=='@imageFilename' or key=='@imageWidth' or key=='@imageHeight'):\n",
    "            continue\n",
    "        if(key==\"TextRegion\"):\n",
    "            if(type(anno[\"TextRegion\"]) == list):\n",
    "                for j in range(len(anno[\"TextRegion\"])):\n",
    "                    text=anno[\"TextRegion\"][j]\n",
    "                    xmin=width\n",
    "                    ymin=height\n",
    "                    xmax=0\n",
    "                    ymax=0\n",
    "                    if(text[\"Coords\"]):\n",
    "                        if(text[\"Coords\"][\"Point\"]):\n",
    "                            for k in range(len(text[\"Coords\"][\"Point\"])):\n",
    "                                coordinates=anno[\"TextRegion\"][j][\"Coords\"][\"Point\"][k]\n",
    "                                xmin= min(xmin, int(coordinates['@x']));\n",
    "                                ymin= min(ymin, int(coordinates['@y']));\n",
    "                                xmax= min(max(xmax, int(coordinates['@x'])), width);\n",
    "                                ymax= min(max(ymax, int(coordinates['@y'])), height);\n",
    "                            if('@type' in text.keys()):    \n",
    "                                label_str+= str(xmin)+' '+str(ymin)+' '+str(xmax)+' '+str(ymax)+' '+text['@type']+' '\n",
    "                                if(xmax<=xmin or ymax<=ymin):\n",
    "                                    continue\n",
    "                                tbox=[];\n",
    "                                tbox.append(xmin)\n",
    "                                tbox.append(ymin)\n",
    "                                tbox.append(xmax)\n",
    "                                tbox.append(ymax)\n",
    "                                tbox.append(text['@type'])\n",
    "                                box.append(tbox)\n",
    "            else:\n",
    "                text=anno[\"TextRegion\"]\n",
    "                xmin=width\n",
    "                ymin=height\n",
    "                xmax=0\n",
    "                ymax=0\n",
    "                if(text[\"Coords\"]):\n",
    "                    if(text[\"Coords\"][\"Point\"]):\n",
    "                        for k in range(len(text[\"Coords\"][\"Point\"])):\n",
    "                            coordinates=anno[\"TextRegion\"][\"Coords\"][\"Point\"][k]\n",
    "                            xmin= min(xmin, int(coordinates['@x']));\n",
    "                            ymin= min(ymin, int(coordinates['@y']));\n",
    "                            xmax= min(max(xmax, int(coordinates['@x'])), width);\n",
    "                            ymax= min(max(ymax, int(coordinates['@y'])), height);\n",
    "                        if('@type' in text.keys()):    \n",
    "                            label_str+= str(xmin)+' '+str(ymin)+' '+str(xmax)+' '+str(ymax)+' '+text['@type']+' '\n",
    "                            if(xmax<=xmin or ymax<=ymin):\n",
    "                                continue\n",
    "                            tbox=[];\n",
    "                            tbox.append(xmin)\n",
    "                            tbox.append(ymin)\n",
    "                            tbox.append(xmax)\n",
    "                            tbox.append(ymax)\n",
    "                            tbox.append(text['@type'])\n",
    "                            box.append(tbox)\n",
    "        \n",
    "        else:\n",
    "            val=\"\"\n",
    "            if(key=='GraphicRegion'):\n",
    "                val=\"graphics\"\n",
    "                augment=True\n",
    "            elif(key=='ImageRegion'):\n",
    "                val=\"image\"\n",
    "            elif(key=='NoiseRegion'):\n",
    "                val=\"noise\"\n",
    "                augment=True\n",
    "            elif(key=='ChartRegion'):\n",
    "                val=\"chart\"\n",
    "                augment=True\n",
    "            elif(key=='TableRegion'):\n",
    "                val=\"table\"\n",
    "                augment=True\n",
    "            elif(key=='SeparatorRegion'):\n",
    "                val=\"separator\"\n",
    "            elif(key=='MathsRegion'):\n",
    "                val=\"maths\"\n",
    "                augment=True\n",
    "            elif(key=='LineDrawingRegion'):\n",
    "                val=\"linedrawing\"\n",
    "                augment=True\n",
    "            else:\n",
    "                val=\"frame\"\n",
    "                augment=True\n",
    "\n",
    "            \n",
    "            if(type(anno[key]) == list):\n",
    "                for j in range(len(anno[key])):\n",
    "                    text=anno[key][j]\n",
    "                    xmin=width\n",
    "                    ymin=height\n",
    "                    xmax=0\n",
    "                    ymax=0\n",
    "                    if(text[\"Coords\"]):\n",
    "                        if(text[\"Coords\"][\"Point\"]):\n",
    "                            for k in range(len(text[\"Coords\"][\"Point\"])):\n",
    "                                coordinates=anno[key][j][\"Coords\"][\"Point\"][k]\n",
    "                                xmin= min(xmin, int(coordinates['@x']));\n",
    "                                ymin= min(ymin, int(coordinates['@y']));\n",
    "                                xmax= min(max(xmax, int(coordinates['@x'])), width);\n",
    "                                ymax= min(max(ymax, int(coordinates['@y'])), height);\n",
    "                        label_str+= str(xmin)+' '+str(ymin)+' '+str(xmax)+' '+str(ymax)+' '+ val +' '\n",
    "                        if(xmax<=xmin or ymax<=ymin):\n",
    "                            continue\n",
    "                        tbox=[];\n",
    "                        tbox.append(xmin)\n",
    "                        tbox.append(ymin)\n",
    "                        tbox.append(xmax)\n",
    "                        tbox.append(ymax)\n",
    "                        tbox.append(val)\n",
    "                        box.append(tbox)\n",
    "            else:\n",
    "                text=anno[key]\n",
    "                xmin=width\n",
    "                ymin=height\n",
    "                xmax=0\n",
    "                ymax=0\n",
    "                if(text[\"Coords\"]):\n",
    "                    if(text[\"Coords\"][\"Point\"]):\n",
    "                        for k in range(len(text[\"Coords\"][\"Point\"])):\n",
    "                            coordinates=anno[key][\"Coords\"][\"Point\"][k]\n",
    "                            xmin= min(xmin, int(coordinates['@x']));\n",
    "                            ymin= min(ymin, int(coordinates['@y']));\n",
    "                            xmax= min(max(xmax, int(coordinates['@x'])), width);\n",
    "                            ymax= min(max(ymax, int(coordinates['@y'])), height);  \n",
    "                        label_str+= str(xmin)+' '+str(ymin)+' '+str(xmax)+' '+str(ymax)+' '+val+' '\n",
    "                        if(xmax<=xmin or ymax<=ymin):\n",
    "                            continue\n",
    "                        tbox=[];\n",
    "                        tbox.append(xmin)\n",
    "                        tbox.append(ymin)\n",
    "                        tbox.append(xmax)\n",
    "                        tbox.append(ymax)\n",
    "                        tbox.append(val)\n",
    "                        box.append(tbox)\n",
    "\n",
    "    label_str=label_str[:-1]\n",
    "    combined.append([fname, label_str])\n",
    "\n",
    "    if(augment):\n",
    "        augmentData(fname, box)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(combined, columns = ['ID', 'Label']);\n",
    "df.to_csv(final_root_dir + \"/train_labels.csv\", index=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"Monk_Object_Detection/1_gluoncv_finetune/lib/\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detector_prototype import Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtf = Detector();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"Document_Layout_Analysis/\";\n",
    "img_dir = \"Images/\";\n",
    "anno_file = \"train_labels.csv\";\n",
    "batch_size=8;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtf.Dataset(root, img_dir, anno_file, batch_size=batch_size);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Available models\n",
    "    ssd_300_vgg16_atrous_coco\n",
    "    ssd_300_vgg16_atrous_voc\n",
    "    ssd_512_vgg16_atrous_coco\n",
    "    ssd_512_vgg16_atrous_voc\n",
    "    ssd_512_resnet50_v1_coco\n",
    "    ssd_512_resnet50_v1_voc\n",
    "    ssd_512_mobilenet1.0_voc\n",
    "    ssd_512_mobilenet1.0_coco\n",
    "    yolo3_darknet53_voc\n",
    "    yolo3_darknet53_coco\n",
    "    yolo3_mobilenet1.0_voc\n",
    "    yolo3_mobilenet1.0_coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vgg16 architecture, with atrous convolutions, pretrained on COCO dataset is used for this task\n",
    "pretrained = True;         \n",
    "gpu=True;\n",
    "model_name = \"ssd_512_vgg16_atrous_coco\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtf.Model(model_name, use_pretrained=pretrained, use_gpu=gpu);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtf.Set_Learning_Rate(0.003);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=30;\n",
    "params_file = \"saved_model.params\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtf.Train(epochs, params_file);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"Monk_Object_Detection/1_gluoncv_finetune/lib/\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference_prototype import Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ssd_512_vgg16_atrous_coco\";\n",
    "params_file = \"saved_model.params\";\n",
    "class_list = [\"paragraph\", \"heading\", \"credit\", \"footer\", \"drop-capital\", \"floating\", \"noise\", \"maths\", \"header\", \"caption\", \"image\", \"linedrawing\", \"graphics\", \"fname\", \"page-number\", \"chart\", \"separator\", \"table\"];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtf = Infer(model_name, params_file, class_list, use_gpu=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name = \"Test_Images/test1.jpg\"; \n",
    "visualize = True;\n",
    "thresh = 0.3;\n",
    "output = gtf.run(img_name, visualize=visualize, thresh=thresh);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name = \"Test_Images/test2.jpg\"; \n",
    "visualize = True;\n",
    "thresh = 0.3;\n",
    "output = gtf.run(img_name, visualize=visualize, thresh=thresh);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name = \"Test_Images/test3.jpg\"; \n",
    "visualize = True;\n",
    "thresh = 0.4;\n",
    "output = gtf.run(img_name, visualize=visualize, thresh=thresh);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "SSD512 produces outputs with very high confidence, a lot of them being 0.9+. It was also the only model which was able to identify footer and noises like division lines in the document. But it was also producing repetitive or incorrect headings such as ‘floating’ in the 2nd example (extra box with incorrect label), and graphics and paragraph in the third (2 boxes with different labels for the same region).\n",
    "\n",
    "If these small details like footer, separator, etc. are crucial for your work and the focus is more on bounding box prediction than classification, go for SSD512. It should also be considered that gluoncv-finetune pipeline of Monk AI (which has been used for SSD512) also provides architectures which are pre-trained on various other datasets, such as COCO dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
